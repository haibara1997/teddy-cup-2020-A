{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看原始数的缺失值情况 输入：原始数据 输出：原始数据的具体缺失信息\n",
    "daily_data = pd.read_csv('/kaggle/input/teddycup/A题全部数据/日数据.csv',encoding='GBK')\n",
    "yearly_data =  pd.read_csv('/kaggle/input/teddycup/A题全部数据/年数据.csv',encoding='GBK')\n",
    "fun_data = pd.read_csv('/kaggle/input/teddycup/A题全部数据/基础数据.csv',encoding='GBK')\n",
    "print('日数据形状：{}'.format(daily_data.shape))\n",
    "print('年数据形状:{}'.format(yearly_data.shape))\n",
    "print('基础数据形状：{}'.format(fun_data.shape))\n",
    "print(daily_data.columns) #日数据的61个变量\n",
    "for x in list(yearly_data.columns): #年数据的362个变量\n",
    "    print(x)\n",
    "print(fun_data.columns)\n",
    "#年数据的缺失值情况\n",
    "#(24262, 362)年数据的shape\n",
    "print(yearly_data.isnull().any())\n",
    "print(\"其中有这么多列是有空缺数据的：\")\n",
    "print(sum(yearly_data.isnull().any()))\n",
    "#建立一个记录缺失值比例的DataFrame\n",
    "yearly_null_col_ratio = pd.Series([],name='年数据缺失值比例')\n",
    "print('年数据分别有以下几列是有空缺值的(每列总共有24262个数据)')\n",
    "print('有缺失值的列','\\t\\t\\t\\t\\t','缺失值的数量''\\t','缺失值数量占总体的百分比')\n",
    "tell_if_null = yearly_data.isnull().any()\n",
    "null_columns = tell_if_null[tell_if_null==1]\n",
    "i=0#计数器而已\n",
    "for null_column in null_columns.index:\n",
    "    num_of_nulls = sum(yearly_data[null_column].isnull())\n",
    "    print(\"{:<40}\\t{:>10}\\t{:>10.2%}\".format(null_column,num_of_nulls,(num_of_nulls/24262)))\n",
    "    yearly_null_col_ratio[i] = num_of_nulls/24262\n",
    "    i += 1\n",
    "#日数据的缺失值情况\n",
    "#(5899132, 61)日数据的shape\n",
    "print(daily_data.isnull().any())\n",
    "print(\"其中有这么多列是有空缺数据的：\")\n",
    "print(sum(daily_data.isnull().any()))\n",
    "#建立一个记录缺失值比例的DataFrame\n",
    "daily_null_col_ratio = pd.Series([],name='日数据缺失值比例')\n",
    "print('日数据分别有以下几列是有空缺值的(每列总共有5899132个数据)')\n",
    "print('\\t有缺失值的列','\\t\\t','缺失值的数量''\\t\\t','缺失值数量占总体的百分比')\n",
    "tell_if_null = daily_data.isnull().any()\n",
    "null_columns = tell_if_null[tell_if_null==1]\n",
    "i = 0#计数器而已\n",
    "for null_column in null_columns.index:\n",
    "    num_of_nulls = sum(daily_data[null_column].isnull())\n",
    "    print(\"{:^20}\\t{:^10d}\\t{:>10.2%}\".format(null_column,num_of_nulls,num_of_nulls/5899132))\n",
    "    daily_null_col_ratio[i] = num_of_nulls/5899132\n",
    "    i += 1\n",
    "#基础数据的缺失值情况\n",
    "#(3466, 4) 基础数据的shape\n",
    "print(fun_data.isnull().any())\n",
    "print(\"其中有这么多列是有空缺数据的：\")\n",
    "print(sum(fun_data.isnull().any()))\n",
    "print('基础数据分别有以下几列是有空缺值的(每列总共有5899132个数据)')\n",
    "print('\\t有缺失值的列','\\t\\t','缺失值的数量''\\t\\t','缺失值数量占总体的百分比')\n",
    "tell_if_null = fun_data.isnull().any()\n",
    "null_columns = tell_if_null[tell_if_null==1]\n",
    "for null_column in null_columns.index:\n",
    "    num_of_nulls = sum(fun_data[null_column].isnull())\n",
    "    print(\"{:^20}\\t{:^10d}\\t{:>10.2%}\".format(null_column,num_of_nulls,num_of_nulls/3466))\n",
    "#做缺失值比例的图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗年数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#读取年数据\n",
    "data_year = pd.read_csv('年数据.csv' , encoding='GBK')\n",
    "print(data_year.head())\n",
    "\n",
    "#删除未上市年份的整行数据\n",
    "data_year=data_year.dropna(thresh=4)\n",
    "data_year.index=range(22552)\n",
    "\n",
    "#删除方差为零的整列数据\n",
    "data_year.drop(labels = '会计区间', axis = 1, inplace=True)\n",
    "data_year.drop(labels = '合并标志，1-合并，2-母公司', axis = 1, inplace=True)\n",
    "data_year.drop(labels = '会计准则', axis = 1, inplace=True)\n",
    "data_year.drop(labels = '货币代码', axis = 1, inplace=True)\n",
    "\n",
    "df = data_year.iloc[:,0:353]\n",
    "#各变量中缺失值的比例\n",
    "r=df.isnull().sum(axis = 0)/df.shape[0]\n",
    "r1=r[r>=0.2]\n",
    "\n",
    "r.plot(kind='hist')\n",
    "plt.xlabel('缺失值占比') \n",
    "plt.ylabel('变量个数') \n",
    "\n",
    "#删除缺失值较多的列 \n",
    "for i in range(len(r1)):\n",
    "    df.drop(labels = r1.index[i], axis = 1, inplace=True)\n",
    "\n",
    "#按股票编号分组，求出每股各变量的中位数\n",
    "id_grouped = df.groupby(['股票编号'])\n",
    "med=pd.DataFrame(np.zeros((3466,216)))\n",
    "for i in range(216):\n",
    "    a=id_grouped.aggregate({df.columns[i]:np.median})\n",
    "    med.iloc[:,i]=id_grouped.aggregate({df.columns[i]:np.median})\n",
    "    a.index=range(3466)\n",
    "    med.iloc[:,i]=a.values\n",
    "\n",
    "med1=med.dropna()\n",
    "\n",
    "med0=med.replace(to_replace=0,value = np.nan)\n",
    "med2=data0.dropna()\n",
    "\n",
    "med=med.fillna(0)\n",
    "\n",
    "#查找缺失值的位置，用每股各变量的中位数填充\n",
    "dot=np.where(np.isnan(df))\n",
    "x=dot[0]\n",
    "y=dot[1]\n",
    "for i in range(len(x)):\n",
    "    df.iloc[x[i],y[i]]=med.iloc[df.iloc[x[i],0]-1,y[i]]\n",
    "    \n",
    "data_year.iloc[:,0:216]=df\n",
    "data_year.drop(data_year.iloc[:,216:353].columns, axis = 1, inplace=True)\n",
    "data_year.to_csv('out.csv', mode='w', header=True, index=False, encoding='GBK') \n",
    "\n",
    "list1=[]\n",
    "for i in med1[0]:\n",
    "    list1.extend(df.loc[year['股票编号']==i].index.tolist())\n",
    "\n",
    "data_year1=data_year.loc[list1,:]\n",
    "data_year1.to_csv('out1.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "\n",
    "list2=[]\n",
    "for i in med2[0]:\n",
    "    list2.extend(df.loc[year['股票编号']==i].index.tolist())\n",
    "\n",
    "data_year2=data_year.loc[list2,:]\n",
    "data_year2.to_csv('out2.csv', mode='w', header=True, index=False, encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清洗日数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#读取日数据\n",
    "data_day = pd.read_csv('日数据.csv' , encoding='GBK')\n",
    "print(data_day.head())\n",
    "\n",
    "#各变量中缺失值的比例\n",
    "r=data_day.isnull().sum(axis = 0)/data_day.shape[0]\n",
    "r1=r[r>=0.37]\n",
    "r.plot(kind='hist')\n",
    "plt.xlabel('缺失值占比') \n",
    "plt.ylabel('变量个数')\n",
    "\n",
    "#删除缺失值较多的列 \n",
    "for i in range(len(r1)):\n",
    "    data_day.drop(labels = r1.index[i], axis = 1, inplace=True)\n",
    "    \n",
    "#按股票编号分组，求出每股各变量的中位数，缺失值用0填充\n",
    "id_grouped = data_day.groupby(['股票编号','年','月'])\n",
    "data=pd.DataFrame(np.zeros((291144,40)))\n",
    "for i in range(0,40):\n",
    "    a=id_grouped.aggregate({data_day.columns[i]:np.median})\n",
    "    a.index=range(291144)\n",
    "    data.iloc[:,i]=a.values\n",
    "\n",
    "data=data.fillna(0)\n",
    "\n",
    "#查找缺失值的位置，用每股各变量的中位数填充\n",
    "dot=np.where(np.isnan(data_day))\n",
    "x=dot[0]\n",
    "y=dot[1]\n",
    "for i in range(len(x)):\n",
    "    data_day.iloc[x[i],y[i]]=data.iloc[data.loc[data[0] == data_day.iloc[x[i],0]].loc[data[1] == data_day.iloc[x[i],1]].loc[data[2] == data_day.iloc[x[i],2]].index[0],y[i]]\n",
    "    \n",
    "data_day.to_csv('out.csv', mode='w', header=True, index=False, encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日数据 输入：原来的日数据 输出：加好是否高转送标签以后的日数据，需要观察发生高转送股票的日数据的特点\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(daily_null_col_ratio,height=1)\n",
    "plt.title('daily_null_col_ratio')\n",
    "daily_null_col_ratio.plot.barh(title='daily_null_col_ratio')\n",
    "yearly_null_col_ratio.hist()\n",
    "yearly_null_col_ratio.plot.barh(title='yearly_null_col_ratio')\n",
    "\n",
    "reference_for_daily_data = yearly_data[['股票编号','年份（年末）','是否高转送']]\n",
    "print(reference_for_daily_data)\n",
    "#创造一列空的数据，用来判断日数据是否高送转\n",
    "daily_data_if_gsz = pd.Series(np.zeros(daily_data.shape[0]),name='是否高送转')\n",
    "#将这列数据并进日数据\n",
    "daily_data = pd.concat([daily_data,daily_data_if_gsz],axis=1)\n",
    "i=0#计数器\n",
    "while i< reference_for_daily_data.shape[0]:\n",
    "    #reference_for_daily_data[['股票编号','年份（年末）']][reference_for_daily_data['股票编号']==1]\n",
    "    t = reference_for_daily_data[['股票编号','年份（年末）','是否高转送']].iloc[i,:]\n",
    "    if t[2] == 0:\n",
    "        pass\n",
    "    else:\n",
    "        #selected_cols = daily_data[['股票编号','年','是否高送转']][(daily_data['股票编号']==t[0]) & (daily_data['年']==t[1])]\n",
    "        #daily_data[(daily_data['股票编号']==t[0]) & (daily_data['年']==t[1])]['是否高送转'] = 1 #[['股票编号','年','是否高送转']]\n",
    "        target_indexs = daily_data[(daily_data['股票编号']==t[0]) & (daily_data['年']==t[1])]['是否高送转'].index\n",
    "        daily_data.iloc[target_indexs,-1] = 1\n",
    "    i += 1\n",
    "daily_data['是否高送转'].sum()/daily_data.shape[0]\n",
    "#验证是否加标签成功\n",
    "daily_data[(daily_data['股票编号']==2) & (daily_data['年']==7)]['是否高送转']\n",
    "daily_data[(daily_data['股票编号']==41) & (daily_data['年']==7)]['是否高送转']\n",
    "#经验证，加标签成功\n",
    "#输出加好标签的日数据\n",
    "daily_data.to_csv('加好标签的日数据.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "test_df = pd.DataFrame([1,2,3,4,5])\n",
    "test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算日数据几列变量一年期间的均值、方差、极差、峰度，并将这些特征根据股票编号和年份并到年数据里面去\n",
    "#输入年数据，日数据，输出加好日数据的特征的年数据\n",
    "#给年数据添加几个特征，并全都初始化为0\n",
    "yearly_data['收盘价均值'] = 0.0\n",
    "yearly_data['收盘价极差'] = 0.0\n",
    "yearly_data['收盘价方差'] = 0.0\n",
    "yearly_data['收盘价峰度'] = 0.0\n",
    "yearly_data['成交量均值'] = 0.0\n",
    "yearly_data['成交量极差'] = 0.0\n",
    "yearly_data['成交量方差'] = 0.0\n",
    "yearly_data['成交量峰度'] = 0.0\n",
    "yearly_data['成交金额均值'] = 0.0\n",
    "yearly_data['成交金额极差'] = 0.0\n",
    "yearly_data['成交金额方差'] = 0.0\n",
    "yearly_data['成交金额峰度'] = 0.0\n",
    "#日数据的特征提取出来，并到年数据上面去\n",
    "stock_id_cans = range(1,3467) #[1,2,...,3466]\n",
    "year_id_cans = range(1,8) #[1,2,3,4,5,6,7]\n",
    "\n",
    "#按照规定的股票编号和规定的年份提取出了我要的一组数据\n",
    "for i in stock_id_cans:\n",
    "    for j in year_id_cans:\n",
    "        if len(yearly_data[(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)]) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            a_series_data = partial_daily_data[(partial_daily_data['股票编号']==i) & (partial_daily_data['年']==j)]\n",
    "\n",
    "            yearly_data['收盘价均值'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['收盘价'].mean()\n",
    "            yearly_data['收盘价极差'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = max(a_series_data['收盘价']) - min(a_series_data['收盘价'])\n",
    "            yearly_data['收盘价方差'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['收盘价'].var()\n",
    "            yearly_data['收盘价峰度'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['收盘价'].kurt()\n",
    "\n",
    "            yearly_data['成交量均值'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['成交量'].mean()\n",
    "            yearly_data['成交量极差'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = max(a_series_data['成交量']) - min(a_series_data['成交量'])\n",
    "            yearly_data['成交量方差'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['成交量'].var()\n",
    "            yearly_data['成交量峰度'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['成交量'].kurt()\n",
    "\n",
    "            yearly_data['成交金额均值'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['成交金额'].mean()\n",
    "            yearly_data['成交金额极差'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = max(a_series_data['成交金额']) - min(a_series_data['成交金额'])\n",
    "            yearly_data['成交金额方差'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['成交金额'].var()\n",
    "            yearly_data['成交金额峰度'][(yearly_data['股票编号']==i) & (yearly_data['年份（年末）']==j)] = a_series_data['成交金额'].kurt()\n",
    "        \n",
    "        #shou_pan_jia_the_mean = a_series_data['收盘价'].mean()\n",
    "        #yearly_data[(yearly_data['股票编号']==1) & (yearly_data['年份（年末）']==1)] #['收盘价的均值']\n",
    "        \n",
    "        #t = yearly_data.copy()\n",
    "        #t[(t['股票编号']==1) & (t['年份（年末）']==1)]['收盘价均值'] = 14.45\n",
    "        #t['收盘价均值'] = 0\n",
    "        #t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将基础数据的特征根据不同行业和是否是次新股，并到年数据里面去 输入年数据，基础数据，输出加好基础数据标签的年数据\n",
    "#收集次新股的股票编号\n",
    "ci_xin_gu_can = [] #只是股票的编号，不是索引编号\n",
    "i = 0\n",
    "while i<len(fun_data['所属概念板块']):\n",
    "    try:\n",
    "        string = fun_data['所属概念板块'][i]\n",
    "        t = string.find('次新股')\n",
    "        if t!=-1:\n",
    "            ci_xin_gu_can.append(i+1)\n",
    "    except:\n",
    "        pass\n",
    "    i += 1\n",
    "\n",
    "#给年数据添加一列专门用来描述是不是次新股\n",
    "yearly_data['是否次新股'] = 0\n",
    "\n",
    "for one_ci_xin_gu in  ci_xin_gu_can:\n",
    "    yearly_data['是否次新股'][(yearly_data['股票编号']==one_ci_xin_gu)] = 1\n",
    "#(2)把行业这个独热变量加到年数据里面去\n",
    "#找出一共有多少种行业，然后\n",
    "all_professions = list(fun_data['所属行业'].drop_duplicates())\n",
    "for one_profession in all_professions:\n",
    "    yearly_data[str(one_profession)] = 0\n",
    "i = 0 #计数器而已\n",
    "while i < len(fun_data):\n",
    "    stock_id,profession = fun_data.iloc[i,[0,2]]\n",
    "    yearly_data[profession][(yearly_data['股票编号']==stock_id)] = 1\n",
    "    i += 1\n",
    "list(yearly_data.columns)\n",
    "yearly_data.to_csv('加上日数据和基础数据信息的年数据.csv', mode='w', header=True, index=False, encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用前一年的数据预测下一年是否发生转送，即把年数据错位一下 输入年数据 输出处理后的年数据，以及第七年的数据\n",
    "a_list = []\n",
    "stock_can = list(yearly_data['股票编号'].drop_duplicates())\n",
    "x = pd.DataFrame(None,columns = x_columns)\n",
    "y = pd.Series(None,name='是否高转送')\n",
    "x_seventh_year = pd.DataFrame(None,columns = x_columns)\n",
    "\n",
    "\n",
    "for i in stock_can:\n",
    "    #yearly_data[yearly_data['股票编号']==i]\n",
    "    if len(yearly_data[yearly_data['股票编号']==i]) == 1:\n",
    "        pass\n",
    "    else:\n",
    "        t_all = yearly_data[yearly_data['股票编号']==i] #temp_all\n",
    "        t_x = t_all.drop(columns='是否高转送') #temp_all_x #t_x.iloc[-1,:] #x_seventh_year\n",
    "        \n",
    "        x_seventh_year = x_seventh_year.append(t_x.iloc[-1,:])\n",
    "\n",
    "        t_x = t_x.iloc[:-1,:]\n",
    "        t_y = t_all['是否高转送'][1:]\n",
    "        x = pd.concat([x,t_x], ignore_index=True) #必须索引了，因为数据错位了，否则没法对齐\n",
    "        y = pd.concat([y,t_y], ignore_index=True)\n",
    "        \n",
    "        #yearly_data[yearly_data['股票编号']==4]['是否高转送']\n",
    "final_data = pd.concat([x,y],axis=1)\n",
    "final_data.rename(columns={'是否高转送':'第二年是否高转送'},inplace=True)\n",
    "final_data\n",
    "final_data.to_csv('final_data_3.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "x_seventh_year.to_csv('final_data_3_7thyear.csv', mode='w', header=True, index=False, encoding='GBK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给每列加上上一年的同样的自变量(共251个) (因变量) 时序因子的加入 输入上一步处理后的年数据 输出加上时序特征以后的年数据\n",
    "yearly_data = pd.read_csv('/kaggle/input/teddycup55/final_data_1.csv',encoding='GBK')\n",
    "#drop掉三列日期\n",
    "yearly_data.drop(columns=['高转送预案公告日','高转送股权登记日','高转送除权日'],inplace=True)\n",
    "yearly_data.info()\n",
    "#创建一个新建列名的列表，即都是含有（上一年）的list\n",
    "new_variables_list = []\n",
    "for x_column in x_columns2:\n",
    "    new_variables_list.append(str(x_column+'(上一年)'))\n",
    "   \n",
    "#初始化新加的变量\n",
    "for new_variable in new_variables_list:\n",
    "    yearly_data[new_variable] = 0.0\n",
    "\n",
    "yearly_data.shape #251-3+251-3+1 = 497\n",
    "#开始填上一年的数据\n",
    "for i in range(len(yearly_data)):\n",
    "    t_year = yearly_data.iloc[i,:]['年份（年末）']\n",
    "    t_stock = yearly_data.iloc[i,:]['股票编号']\n",
    "    if len(yearly_data[(yearly_data['年份（年末）']==t_year-1) & (yearly_data['股票编号']==t_stock)]) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        index_current = (yearly_data[new_variables_list][(yearly_data['年份（年末）']==t_year) & (yearly_data['股票编号']==t_stock)].index)[0]\n",
    "        #yearly_data[new_variables_list][(yearly_data['年份（年末）']==t_year) & (yearly_data['股票编号']==t_stock)] = yearly_data[x_columns][(yearly_data['年份（年末）']==t_year-1) & (yearly_data['股票编号']==t_stock)]\n",
    "        yearly_data.iloc[index_current,249:] = list(yearly_data.iloc[index_current-1,:248])\n",
    "#删除每组序列数据的第一个\n",
    "stock_can = yearly_data['股票编号'].drop_duplicates()\n",
    "wanted_delete_index_can = []\n",
    "for i in list(stock_can):\n",
    "    t = yearly_data[(yearly_data['股票编号']==i)].iloc[0,:].name\n",
    "    wanted_delete_index_can.append(t)\n",
    "yearly_data.drop(index=wanted_delete_index_can,inplace=True)\n",
    "yearly_data.to_csv('final_data_1_5-5(2).csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "#给7th_year的数据加上前一年的时间序列特征\n",
    "yearly_data1 = pd.read_csv('/kaggle/input/teddycup55/final_data_1.csv',encoding='GBK')\n",
    "yearly_data2 = pd.read_csv('/kaggle/input/teddycup55/final_data_1_7thyear.csv',encoding='GBK')\n",
    "#drop掉三列日期\n",
    "yearly_data1.drop(columns=['高转送预案公告日','高转送股权登记日','高转送除权日'],inplace=True)\n",
    "yearly_data2.drop(columns=['高转送预案公告日','高转送股权登记日','高转送除权日'],inplace=True)\n",
    "#创建一个新建列名的列表，即都是含有（上一年）的list\n",
    "new_variables_list = []\n",
    "for x_column in x_columns2:\n",
    "    new_variables_list.append(str(x_column+'(上一年)'))\n",
    "   \n",
    "#初始化新加的变量\n",
    "for new_variable in new_variables_list:\n",
    "    yearly_data2[new_variable] = 0.0\n",
    "\n",
    "yearly_data2.shape #251-3+251-3+1 = 497 #497-1=496 减去那个因变量\n",
    "#开始填上一年的数据\n",
    "for i in range(len(yearly_data2)):\n",
    "    t_stock_id = yearly_data2.iloc[i,0]\n",
    "    yearly_data2.iloc[i,248:] = (yearly_data1[(yearly_data1['股票编号']==t_stock_id) & (yearly_data1['年份（年末）']==6)].drop(columns=['第二年是否高转送'])).values.reshape(-1,)\n",
    "yearly_data2.to_csv('final_data_1_5-5_7thyear(2).csv', mode='w', header=True, index=False, encoding='GBK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准化 输入上一步得到的数据 输出标准化以后的数据\n",
    "#用0来填每股送转的两列nan(每股送转、 每股送转(上一年) )\n",
    "file_list = [final_data_1_7thyear,final_data_2_7thyear,final_data_3_7thyear,final_data_1,final_data_2,final_data_3]\n",
    "for file in files_list:\n",
    "    drop_columns1 = list(file.iloc[:,[248,249]].columns) #股票编号(上一年) 年份(年末)(上一年)\n",
    "    drop_columns2 = list(file.iloc[:,-19:].columns) #是否是次新股(上一年) 房地产业(上一年) 制造业(上一年) ...... 教育(上一年)\n",
    "    file.fillna(0,inplace=True)\n",
    "final_data_1_7thyear.iloc[:,-19:]\n",
    "#drop掉多余的几列\n",
    "files_list_7th = [final_data_1_7thyear,final_data_2_7thyear,final_data_3_7thyear]\n",
    "files_list     = [final_data_1,final_data_2,final_data_3]\n",
    "#drop掉7th的数据\n",
    "final_data_3_7thyear.iloc[:,[248,249]] #股票编号(上一年) 年份（年末）(上一年)\n",
    "final_data_3_7thyear.iloc[:,-19:] #是否是次新股(上一年) 房地产业(上一年) 制造业(上一年) ...... 教育(上一年)\n",
    "for file in files_list_7th:\n",
    "    drop_columns1 = list(file.iloc[:,[248,249]].columns) #股票编号(上一年) 年份(年末)(上一年)\n",
    "    drop_columns2 = list(file.iloc[:,-19:].columns) #是否是次新股(上一年) 房地产业(上一年) 制造业(上一年) ...... 教育(上一年)\n",
    "    file.drop(columns=drop_columns1+drop_columns2,inplace=True)\n",
    "#再drop掉剩下的数据\n",
    "final_data_3.iloc[:,[249,250]] #股票编号(上一年) 年份（年末）(上一年)\n",
    "final_data_3.iloc[:,-19:]\n",
    "for file in files_list:\n",
    "    drop_columns1 = list(file.iloc[:,[249,250]].columns) #股票编号(上一年) 年份(年末)(上一年)\n",
    "    drop_columns2 = list(file.iloc[:,-19:].columns) #是否是次新股(上一年) 房地产业(上一年) 制造业(上一年) ...... 教育(上一年)\n",
    "    file.drop(columns=drop_columns1+drop_columns2,inplace=True)\n",
    "final_data_1_7thyear.to_csv('1_7th_未标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_2_7thyear.to_csv('2_7th_未标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_3_7thyear.to_csv('3_7th_未标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_1.to_csv('1_未标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_2.to_csv('2_未标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_3.to_csv('3_未标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "#标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "files_list_7th = [final_data_1_7thyear,final_data_2_7thyear,final_data_3_7thyear]\n",
    "files_list     = [final_data_1,final_data_2,final_data_3]\n",
    "#先标准化前六年的data\n",
    "for file in files_list:\n",
    "    std1 = StandardScaler()\n",
    "    file.iloc[:,2:229] = std1.fit_transform(file.iloc[:,2:229])\n",
    "    std2 = StandardScaler()\n",
    "    file.iloc[:,249:] = std2.fit_transform(file.iloc[:,249:])\n",
    "#标准化第七年的data\n",
    "for file in files_list_7th:\n",
    "    std1 = StandardScaler()\n",
    "    file.iloc[:,2:229] = std1.fit_transform(file.iloc[:,2:229])\n",
    "    std2 = StandardScaler()\n",
    "    file.iloc[:,248:] = std2.fit_transform(file.iloc[:,248:])\n",
    "\n",
    "final_data_1_7thyear.to_csv('1_7th_标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_2_7thyear.to_csv('2_7th_标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_3_7thyear.to_csv('3_7th_标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_1.to_csv('1_标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_2.to_csv('2_标准化.csv', mode='w', header=True, index=False, encoding='GBK')\n",
    "final_data_3.to_csv('3_标准化.csv', mode='w', header=True, index=False, encoding='GBK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建模    输入：标准化以后的数据 输出：最终模型\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.externals import joblib\n",
    "can = [\n",
    "'收盘价均值(上一年)','每股送转','基本每股收益','基本每股收益同必增长(%)',\n",
    "'投资活动产生的现金流量净额占必(%)','总资产相对年初增长(%)',\n",
    "'经营活动产生的现金流量净额占必(%)(上一年)','股权自由现金流量','每股盈余公积(元/股)(上一年)',\n",
    "'收盘价峰度','实收资本(或股本)','每股净资产相对年初增长(%)','净资产相对年初增长(%)(上一年)',\n",
    "'经营活动现金流量净额/净债务','收盘价峰度(上一年)','是否次新股','第二年是否高转送']\n",
    "can.append('第二年是否高转送')\n",
    "data1 = pd.read_csv('/kaggle/input/teddy-cup-5-6-afternoon1/加了时序以后可以直接用的数据-5-6上午/标准化版/3_标准化.csv',encoding='GBK')\n",
    "data1 = data1[can]\n",
    "x = data1.drop(columns=['第二年是否高转送'])\n",
    "y = data1['第二年是否高转送']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=391)\n",
    "#随机森林\n",
    "rnd_clf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                       class_weight={0: 0.5896246764452114,\n",
    "                                     1: 3.289410348977136},\n",
    "                       criterion='gini', max_depth=5, max_features='auto',\n",
    "                       max_leaf_nodes=8, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=446,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "\n",
    "y_predicted = rnd_clf.predict(X_train)\n",
    "print(classification_report(y_train,y_predicted,target_names=['0','1']))\n",
    "\n",
    "y_predicted = rnd_clf.predict(X_test)\n",
    "print(classification_report(y_test,y_predicted,target_names=['0','1']))\n",
    "#逻辑回归\n",
    "log_reg = LogisticRegression(class_weight='balanced',max_iter=10000)\n",
    "log_reg.fit(X_train,y_train)\n",
    "y_predicted = log_reg.predict(X_train)\n",
    "print(classification_report(y_train,y_predicted,target_names=['0','1']))\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "print(classification_report(y_test,y_predicted,target_names=['0','1']))\n",
    "#catboost\n",
    "# Initialize data\n",
    "#cat_features = [0,1,2]\n",
    "#train_data = [[\"a\",\"b\",1,4,5,6],[\"a\",\"b\",4,5,6,7],[\"c\",\"d\",30,40,50,60]]\n",
    "#train_labels = [1,1,-1]\n",
    "#test_data = [[\"a\",\"b\",2,4,6,8],[\"a\",\"d\",1,4,50,60]]\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "iterations=3000, \n",
    "learning_rate=0.1, \n",
    "depth=4, loss_function='Logloss',class_weights=[1,5])\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_train)\n",
    "print(classification_report(y_train,y_predicted,target_names=['0','1']))\n",
    "y_predicted = model.predict(X_test)\n",
    "print(classification_report(y_test,y_predicted,target_names=['0','1']))\n",
    "#xgboost\n",
    "clf = XGBClassifier(\n",
    "silent=0 ,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "#nthread=4,# cpu 线程数 默认最大\n",
    "learning_rate= 0.3, # 如同学习率\n",
    "min_child_weight=1, \n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "max_depth=5, # 构建树的深度，越大越容易过拟合\n",
    "gamma=0,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "subsample=1, # 随机采样训练样本 训练实例的子采样比\n",
    "max_delta_step=1,#最大增量步长，我们允许每个树的权重估计。\n",
    "colsample_bytree=1, # 生成树时进行的列采样 \n",
    "reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "#reg_alpha=0, # L1 正则项参数\n",
    "#scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "objective= 'binary:logistic', #多分类的问题 指定学习任务和相应的学习目标\n",
    "#num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "n_estimators=220, #树的个数\n",
    "seed=1000, #随机种子\n",
    "eval_metric= 'auc',\n",
    "scale_pos_weight=8868/1597\n",
    " )\n",
    "\n",
    "clf.fit(X_train,y_train,eval_metric='auc')\n",
    "y_predicted = clf.predict(X_train)\n",
    "print(classification_report(y_train,y_predicted,target_names=['0','1']))\n",
    "y_predicted = clf.predict(X_test)\n",
    "print(classification_report(y_test,y_predicted,target_names=['0','1']))\n",
    "#集成学习\n",
    "rnd_clf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                       class_weight={0: 0.5896246764452114,\n",
    "                                     1: 3.289410348977136},\n",
    "                       criterion='gini', max_depth=5, max_features='auto',\n",
    "                       max_leaf_nodes=8, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=446,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "\n",
    "clf = XGBClassifier(\n",
    "silent=0 ,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "#nthread=4,# cpu 线程数 默认最大\n",
    "learning_rate= 0.3, # 如同学习率\n",
    "min_child_weight=1, \n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "max_depth=5, # 构建树的深度，越大越容易过拟合\n",
    "gamma=0,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "subsample=1, # 随机采样训练样本 训练实例的子采样比\n",
    "max_delta_step=1,#最大增量步长，我们允许每个树的权重估计。\n",
    "colsample_bytree=1, # 生成树时进行的列采样 \n",
    "reg_lambda=1,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "#reg_alpha=0, # L1 正则项参数\n",
    "#scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重\n",
    "objective= 'binary:logistic', #多分类的问题 指定学习任务和相应的学习目标\n",
    "#num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "n_estimators=220, #树的个数\n",
    "seed=1000, #随机种子\n",
    "eval_metric= 'auc',\n",
    "scale_pos_weight=8868/1597\n",
    " )\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "iterations=3000, \n",
    "learning_rate=0.1, \n",
    "depth=4, loss_function='Logloss',class_weights=[1,5])\n",
    "\n",
    "log_reg = LogisticRegression(class_weight='balanced',max_iter=10000)\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "voting_clf = VotingClassifier(estimators=[('rnd_clf',rnd_clf),('xgclf',clf),('catboost',model),('log_reg',log_reg)],voting='soft',weights=[2,5,2,1])  #rf recall高#,weights=[2,5,3,1.5]\n",
    "voting_clf.fit(X_train,y_train)\n",
    "#保存模型\n",
    "y_predicted = voting_clf.predict(X_train)\n",
    "print(classification_report(y_train,y_predicted,target_names=['0','1']))\n",
    "y_predicted = voting_clf.predict(X_test)\n",
    "print(classification_report(y_test,y_predicted,target_names=['0','1']))\n",
    "joblib.dump(voting_clf,'teddycup_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用保存的模型进行预测 输入第七年的数据 输出第八年预测那些股票可能发生高转送\n",
    "#加载模型\n",
    "model = joblib.load('/kaggle/input/11111111111111111111111/teddycup_clf.pkl')\n",
    "can=[\n",
    "'收盘价均值(上一年)',\n",
    "'每股送转',\n",
    "'基本每股收益',\n",
    "'基本每股收益同必增长(%)' ,\n",
    "'投资活动产生的现金流量净额占必(%)', #不在\n",
    "'总资产相对年初增长(%)' , #不在\n",
    "'经营活动产生的现金流量净额占必(%)(上一年)',\n",
    "'股权自由现金流量' ,#不在\n",
    "'每股盈余公积(元/股)(上一年)' ,\n",
    "'收盘价峰度' , #不在\n",
    "'实收资本(或股本)' ,\n",
    "'实收资本(或股本)(上一年)' ,\n",
    "'每股净资产相对年初增长(%)' ,#不在\n",
    "'净资产相对年初增长(%)(上一年)' , #不在\n",
    "'经营活动现金流量净额/净债务' , #不在\n",
    "'收盘价峰度(上一年)',\n",
    "'是否次新股',\n",
    "'稀释每股收益',\n",
    "'每股净资产(元/股)',\n",
    "'每股资本公积(元/股)',\n",
    "'每股未分配利润(元/股)',\n",
    "'每股未分配利润(元/股)(上一年)',\n",
    "'每股净资产(元/股)',\n",
    "'每股营业收入(元/股)'\n",
    "]\n",
    "can.append('第二年是否高转送')\n",
    "my_data = pd.DataFrame([X_train,y_train],) \n",
    "del can[-1]\n",
    "#验证模型是否可用\n",
    "y_train_predicted = model.predict(X_train[can])\n",
    "y_test_predicted = model.predict(X_test[can])\n",
    "print(classification_report(y_train,y_train_predicted,target_names=['0','1']))\n",
    "print(classification_report(y_test,y_test_predicted,target_names=['0','1']))\n",
    "#预测第八年\n",
    "test_8th_year = pd.read_csv('/kaggle/input/teddy-cup-5-6-afternoon1/加了时序以后可以直接用的数据-5-6上午/未标准化版/3_7th_未标准化.csv',encoding='GBK')\n",
    "year_8_predicted = model.predict(test_8th_year[can])\n",
    "predict_8th = pd.DataFrame([test_8th_year['股票编号'],year_8_predicted],index=['股票编号','是否发生了高转送'])\n",
    "predict_8th = predict_8th.T\n",
    "result = predict_8th['股票编号'][predict_8th['是否发生了高转送']==1]\n",
    "result.to_csv('第8年预测会发生高转送的股票.csv',mode='w', header=True, index=False, encoding='GBK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
